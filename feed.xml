<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://esoba.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://esoba.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-18T15:24:58+00:00</updated><id>https://esoba.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Exploring LLM uncertainty</title><link href="https://esoba.github.io/blog/2024/exploring-llm-uncertainty/" rel="alternate" type="text/html" title="Exploring LLM uncertainty"/><published>2024-02-26T17:00:10+00:00</published><updated>2024-02-26T17:00:10+00:00</updated><id>https://esoba.github.io/blog/2024/exploring-llm-uncertainty</id><content type="html" xml:base="https://esoba.github.io/blog/2024/exploring-llm-uncertainty/"><![CDATA[<p><strong>More coming soon</strong></p> <h2 id="intro">Intro</h2> <p>Large Language Models (LLMs) are able to generate content by sampling from a learned probability distribution over possible next tokens. Even though modern LLMs have been trained on most of the internet (multiple times over) and have billions of parameters, there is no real guarentee on an LLMs output. To date, there is no way to fully measure how certain an LLM output is besides human verification. If we can understand what makes an LLM certain/uncertain in its output, it would have huge implications not only for LLM applications, but also for the publics trust in AI as a whole. Below, I’ll explore some concepts and why this is such a hard problem.</p> <h3 id="uncertainty-vs-confidence-vs-calibration">Uncertainty vs Confidence vs Calibration</h3> <h3 id="why-cant-we-use-output-probabilities-to-measure-uncertainty">Why can’t we use output probabilities to measure uncertainty?</h3> <p>Because LLMs output token probabilities, it would seem reasonable that the probability of a sequence would be a good proxy for uncertainty. Or, at the very least, there would be some meaningful way to combine token probabilities to make a good uncertainty metric. With a singular token output, this could work. Consider the following example:</p> <p>Prompt: Given the following headline, determine if it is related to AI. Respond with only a true or false.</p> <p>Headline: Elon Musk sues OpenAI over mission statement</p> <p>Answer: True</p> <p>There was only 1 token output as an answer, so confidence in that token (aka the probability the next token is True given the sequence) represents an LLMs confidence in the answer</p> <p>However, we cannot use that same confidence (probability) metric when we have an answer that is not constrained to a pre-defined set. Consider if the answer to the above question was instead something like:</p> <p>“Yes, this article is related to AI”</p> <p>Semantically, these are the same answer. If I tried to calculate a probability that it would return this exact sequence, I would have to do the following calculation:</p> <table> <tbody> <tr> <td>P(Yes</td> <td>input_sequence)*Prob(,</td> <td>input_sequence + Yes) * Prob(this</td> <td>input_sequence + Yes,) …</td> </tr> </tbody> </table> <p>The probability that this sequence is the answer would not be the same as the probability from the discrete “True” answer above. Sparing the math, the main point is that semantic similarity is not preserved when calculating confidence for a completion that is &gt;1 token long. In many applications for eDiscovery, the completion is never just a simple yes or no. There is typically a reasoning or some other meta data extraction behind it. This makes it difficult to come up with a confidence score based on token probabilities alone. This is also why asking an LLM itself to return a confidence score is not a great solution either. Consider an example where I ask an LLM to return confidence on a scale of 1-10. If it returns 5 with say 40% confidence, were getting a number its 1) not confident in the first place and 2) not in line with the actual proportion for 1-10.</p> <h4 id="token-probability-entropy">Token Probability Entropy</h4> <h3 id="what-evaluation-metrics-has-research-come-up-with">What evaluation metrics has research come up with?</h3> <h3 id="does-asking-an-llm-to-rate-its-own-output-work">Does asking an LLM to rate its own output work?</h3> <p>###</p>]]></content><author><name></name></author><category term="advanced"/><category term="NLP"/><category term="AI-Trust/Safety"/><category term="GenAI"/><summary type="html"><![CDATA[How much can we trust the output of an LLM?]]></summary></entry><entry><title type="html">A note on BERT</title><link href="https://esoba.github.io/blog/2024/a-note-on-bert/" rel="alternate" type="text/html" title="A note on BERT"/><published>2024-02-26T17:00:10+00:00</published><updated>2024-02-26T17:00:10+00:00</updated><id>https://esoba.github.io/blog/2024/a-note-on-bert</id><content type="html" xml:base="https://esoba.github.io/blog/2024/a-note-on-bert/"><![CDATA[<p>Coming Soon!</p>]]></content><author><name></name></author><category term="advanced"/><category term="NLP"/><category term="GenAI"/><summary type="html"><![CDATA[Quick dive into the most common "teaching" model]]></summary></entry><entry><title type="html">Custom fine-tuning example</title><link href="https://esoba.github.io/blog/2024/custom-finetuning-example/" rel="alternate" type="text/html" title="Custom fine-tuning example"/><published>2024-02-26T17:00:10+00:00</published><updated>2024-02-26T17:00:10+00:00</updated><id>https://esoba.github.io/blog/2024/custom-finetuning-example</id><content type="html" xml:base="https://esoba.github.io/blog/2024/custom-finetuning-example/"><![CDATA[<h2 id="intro">Intro</h2> <p>When I first got into NLP, my first couple projects involved building some basic classification, clustering, and RAG algorithms using <a href="https://www.sbert.net/">sentence embeddings</a> or <a href="https://platform.openai.com/docs/guides/embeddings">OpenAI embeddings</a>. I think this was a great start because it made me realize that <a href="">NLP isn’t much different than other ML areas</a>, but as I kept going down the NLP rabbit hole I realized I didn’t truly understand these embeddings.</p> <p>Looking back, I don’t blame myself too much for my lack of understanding because word embeddings are a lot more intuitive. Without diving into specifics (which can be found <a href="">here</a>), word embeddings are created by analyzing the relationships between words and surrounding context in a corpus. The output of a perfect word embedding model are vectors whose geometric similarity (like cosine similarity) are representative of their semantic similarity. Thus, we can expect words with similar meanings to be closer together in the vector space.</p> <p>Even though I understood how word embeddings worked, I struggled with sentences for a couple different reasons. These were some of my qualms:</p> <ul> <li>Can’t there be an infinite number of sentences constructed from words in a corpus?</li> <li>How can you embed paragraphs?</li> <li>Even if you used attention to update vector representations, how do you combine them?</li> </ul> <h2 id="pooling">Pooling</h2> <p>You may have heard of pooling before in other ML contexts, particularly in Computer Vision where pooling refers to downsampling convolutional kernel output (aka feature maps). You perform pooling for a couple different reasons, primarily to:</p> <ul> <li>Reduce dimensionality (makes computation faster)</li> <li>Promote translation invariance (makes network less sensitive to feature location)</li> <li>Abstract/Consolidate features (makes network focus on large patterns vs local details)</li> </ul> <p>In NLP pooling has a different meaning, but can be thought of in the same way as the CV case. Pooling is a method of converting a sequence of word embeddings into a singular embedding that represents primary features of the entire sequence (aka downsampling!). As opposed to Convolutional Neural Networks where pooling is applied after every convolution layer, NLP pooling is only applied at the very end of self-attention blocks. Sentence embeddings are just the output of the pooling operation on context-aware word embeddings (typically from the final attention layer/block).</p> <h2 id="different-pooling-methods">Different pooling methods</h2> <p>Like in CV, there are different pooling methods that can be employed to get a representative embedding that captures the meaning of a sentence.</p> <h2 id="answers-to-my-previous-qualms">Answers to my previous qualms</h2> <ol> <li>Can’t there be an infinite number of sentences constructed from words in a corpus?</li> </ol> <p>Technically yes if we define a sentence to be a length-unrestricted sequence of words (tokens). I can see the confusion where the thought process is “there are an infinite number of combinations so how can an infinite combination fit in one embedding space”. My counter to that is unlike word embeddings, they do not all sit in one space. There is no sentence embedding space like words, it is more so sentence embeddings are a function of embeddings that live in the word embedding space. That function is the pooling operation and can be</p> <ol> <li>How can you embed paragraphs?</li> </ol> <p>Same way you would a sentence (aka pooling). A period (what delimits sentences from each other) is just a token, there is nothing special about it from the perspective of a language model. Assuming the paragraph you want to embed is not larger than the context limit of the language model you are passing it to, the process would be the exact same for a singular sentence.</p> <ol> <li>Even if you used attention to update vector representations, how do you combine them?</li> </ol> <p>Depends on the pooling method you choose. The default method (most common) is just to take the embedding of the classification token (usually <cls> or <s>) from a sequence. The idea is that the classification token captures all of the relevant sequence level information and it's embedding is updated just like the rest of the word embeddings. Another technique is to average the embeddings of all the words in the sentence. There is no consensus on which method to choose, but for models trained on classification tasks its best to just use the cls representation</s></cls></p> <p>My original thought process</p> <h3 id="uncertainty-vs-confidence-vs-calibration">Uncertainty vs Confidence vs Calibration</h3> <h3 id="difference-between-uncertainty-and-hallucination-detection">Difference between uncertainty and hallucination detection</h3> <h3 id="why-cant-we-use-output-probabilities-to-measure-uncertainty">Why can’t we use output probabilities to measure uncertainty?</h3> <p>Because LLMs output token probabilities, it would seem reasonable that the probability of a sequence would be a good proxy for uncertainty. Or, at the very least, there would be some meaningful way to combine token probabilities to make a good uncertainty metric. With a singular token output, this could work. Consider the following example:</p> <blockquote> <p>Enter diagram for LLM output here</p> </blockquote> <p>In this simple example, we ask the LLM a yes or no question. We can look at the probability that the output token is “yes” and say</p> <h4 id="token-probability-entropy">Token Probability Entropy</h4> <p>E</p> <h3 id="what-evaluation-metrics-has-research-come-up-with">What evaluation metrics has research come up with?</h3> <h3 id="does-asking-an-llm-to-rate-its-own-output-work">Does asking an LLM to rate its own output work?</h3> <p>###</p>]]></content><author><name></name></author><category term="advanced"/><category term="NLP"/><category term="GenAI"/><summary type="html"><![CDATA[How can we fine-tune a model more explicitly?]]></summary></entry><entry><title type="html">Difference between Encoder and Decoder Transformers</title><link href="https://esoba.github.io/blog/2024/encoder-vs-decoder-transformers/" rel="alternate" type="text/html" title="Difference between Encoder and Decoder Transformers"/><published>2024-02-26T17:00:10+00:00</published><updated>2024-02-26T17:00:10+00:00</updated><id>https://esoba.github.io/blog/2024/encoder-vs-decoder-transformers</id><content type="html" xml:base="https://esoba.github.io/blog/2024/encoder-vs-decoder-transformers/"><![CDATA[<p><strong>More coming soon</strong></p> <h2 id="intro">Intro</h2> <p>When people first start learning NLP, I think it is really easy to get wrapped up in the transformer architecture and just assume</p> <p>Large Language Models (LLMs) are able to generate content by sampling from a learned probability distribution over possible next tokens. Even though modern LLMs have been trained on most of the internet (multiple times over) and have billions of parameters, there is no real guarentee on an LLMs output. To date, there is no way to fully measure how certain an LLM output is besides human verification. If we can understand what makes an LLM certain/uncertain in its output, it would have huge implications not only for LLM applications, but also for the publics trust in AI as a whole. Below, I’ll explore some concepts and why this is such a hard problem.</p> <h3 id="uncertainty-vs-confidence-vs-calibration">Uncertainty vs Confidence vs Calibration</h3> <p>For the purposes of not confusing terminology, I want to define explicitly what is meant by uncertainty, confidence, and calibration. In statistics/ML, these all have some slight variation on how they are used. To motivate this, consider the following example:</p> <p>Say a language model is given the phrase “The sky is <em>__</em>”. This model will take in the phrase and output a probability distribution over all the possible tokens that should come next in the phrase.</p> <p>This is how I think of them:</p> <p>When I think of confidence, I think of the output probability from a model. For example, if some language model sees the phrase:</p> <p>“The sky is <em>__</em>”</p> <p>It will return a distribution across all tokens in its corpus.</p> <p>When it comes to measuring how sure a model is that the prediction/generation its giving is correct,</p> <h3 id="difference-between-uncertainty-and-hallucination-detection">Difference between uncertainty and hallucination detection</h3> <h3 id="why-cant-we-use-output-probabilities-to-measure-uncertainty">Why can’t we use output probabilities to measure uncertainty?</h3> <p>Because LLMs output token probabilities, it would seem reasonable that the probability of a sequence would be a good proxy for uncertainty. Or, at the very least, there would be some meaningful way to combine token probabilities to make a good uncertainty metric. With a singular token output, this could work. Consider the following example:</p> <blockquote> <p>Enter diagram for LLM output here</p> </blockquote> <p>In this simple example, we ask the LLM a yes or no question. We can look at the probability that the output token is “yes” and say</p> <h4 id="token-probability-entropy">Token Probability Entropy</h4> <p>E</p> <h3 id="what-evaluation-metrics-has-research-come-up-with">What evaluation metrics has research come up with?</h3> <h3 id="does-asking-an-llm-to-rate-its-own-output-work">Does asking an LLM to rate its own output work?</h3> <p>###</p>]]></content><author><name></name></author><category term="advanced"/><category term="NLP"/><category term="GenAI"/><summary type="html"><![CDATA[What architecture are different NLP models using?]]></summary></entry><entry><title type="html">LLMs don’t break ML principles</title><link href="https://esoba.github.io/blog/2024/llms-dont-break-ml-principles/" rel="alternate" type="text/html" title="LLMs don’t break ML principles"/><published>2024-02-26T17:00:10+00:00</published><updated>2024-02-26T17:00:10+00:00</updated><id>https://esoba.github.io/blog/2024/llms-dont-break-ml-principles</id><content type="html" xml:base="https://esoba.github.io/blog/2024/llms-dont-break-ml-principles/"><![CDATA[<p><strong>Finalizing material, will be done soon!</strong> Large Language Models (LLMs) are able to generate content by sampling from a learned probability distribution over possible next tokens. Given a set of modelThe more data that a model is trained on</p>]]></content><author><name></name></author><category term="advanced"/><category term="NLP"/><category term="AI-Trust/Safety"/><category term="GenAI"/><summary type="html"><![CDATA[Relating the basics of ML to the state of the art]]></summary></entry><entry><title type="html">The state of model merging in GenAI</title><link href="https://esoba.github.io/blog/2024/merge-models/" rel="alternate" type="text/html" title="The state of model merging in GenAI"/><published>2024-02-26T17:00:10+00:00</published><updated>2024-02-26T17:00:10+00:00</updated><id>https://esoba.github.io/blog/2024/merge-models</id><content type="html" xml:base="https://esoba.github.io/blog/2024/merge-models/"><![CDATA[<p>Coming Soon!</p>]]></content><author><name></name></author><category term="advanced"/><category term="NLP"/><category term="GenAI"/><summary type="html"><![CDATA[What does it mean to merge models?]]></summary></entry><entry><title type="html">Common modifications to vanilla Transformers</title><link href="https://esoba.github.io/blog/2024/what-changes-are-made-to-transformers/" rel="alternate" type="text/html" title="Common modifications to vanilla Transformers"/><published>2024-02-26T17:00:10+00:00</published><updated>2024-02-26T17:00:10+00:00</updated><id>https://esoba.github.io/blog/2024/what-changes-are-made-to-transformers</id><content type="html" xml:base="https://esoba.github.io/blog/2024/what-changes-are-made-to-transformers/"><![CDATA[<p><strong>More coming soon</strong></p> <h2 id="intro">Intro</h2> <p>When people first start learning NLP, I think it is really easy to get wrapped up in the transformer architecture and just assume</p> <p>Large Language Models (LLMs) are able to generate content by sampling from a learned probability distribution over possible next tokens. Even though modern LLMs have been trained on most of the internet (multiple times over) and have billions of parameters, there is no real guarentee on an LLMs output. To date, there is no way to fully measure how certain an LLM output is besides human verification. If we can understand what makes an LLM certain/uncertain in its output, it would have huge implications not only for LLM applications, but also for the publics trust in AI as a whole. Below, I’ll explore some concepts and why this is such a hard problem.</p> <h3 id="uncertainty-vs-confidence-vs-calibration">Uncertainty vs Confidence vs Calibration</h3> <p>For the purposes of not confusing terminology, I want to define explicitly what is meant by uncertainty, confidence, and calibration. In statistics/ML, these all have some slight variation on how they are used. To motivate this, consider the following example:</p> <p>Say a language model is given the phrase “The sky is <em>__</em>”. This model will take in the phrase and output a probability distribution over all the possible tokens that should come next in the phrase.</p> <p>This is how I think of them:</p> <p>When I think of confidence, I think of the output probability from a model. For example, if some language model sees the phrase:</p> <p>“The sky is <em>__</em>”</p> <p>It will return a distribution across all tokens in its corpus.</p> <p>When it comes to measuring how sure a model is that the prediction/generation its giving is correct,</p> <h3 id="difference-between-uncertainty-and-hallucination-detection">Difference between uncertainty and hallucination detection</h3> <h3 id="why-cant-we-use-output-probabilities-to-measure-uncertainty">Why can’t we use output probabilities to measure uncertainty?</h3> <p>Because LLMs output token probabilities, it would seem reasonable that the probability of a sequence would be a good proxy for uncertainty. Or, at the very least, there would be some meaningful way to combine token probabilities to make a good uncertainty metric. With a singular token output, this could work. Consider the following example:</p> <blockquote> <p>Enter diagram for LLM output here</p> </blockquote> <p>In this simple example, we ask the LLM a yes or no question. We can look at the probability that the output token is “yes” and say</p> <h4 id="token-probability-entropy">Token Probability Entropy</h4> <p>E</p> <h3 id="what-evaluation-metrics-has-research-come-up-with">What evaluation metrics has research come up with?</h3> <h3 id="does-asking-an-llm-to-rate-its-own-output-work">Does asking an LLM to rate its own output work?</h3> <p>###</p>]]></content><author><name></name></author><category term="advanced"/><category term="NLP"/><category term="GenAI"/><summary type="html"><![CDATA[How do LLMs change the transformer architecture?]]></summary></entry><entry><title type="html">A note on imposter syndrome</title><link href="https://esoba.github.io/blog/2024/a-note-on-imposter-syndrome/" rel="alternate" type="text/html" title="A note on imposter syndrome"/><published>2024-02-26T17:00:10+00:00</published><updated>2024-02-26T17:00:10+00:00</updated><id>https://esoba.github.io/blog/2024/a-note-on-imposter-syndrome</id><content type="html" xml:base="https://esoba.github.io/blog/2024/a-note-on-imposter-syndrome/"><![CDATA[<p>Imposter syndrome sucks - it’s that constant “you arent good enough” or “someone else deserves to be in your place” feeling that you get when something good happens to you. I have struggled with imposter syndrome ever since I got to college, and even today I still get this feeling. After years of dealing with it, I want to offer my perspective and how a change in thinking has mitigated some of its effects.</p> <p>For me, imposter syndrome is like all of the super successful people you have ever known are at the top of a mountain. They are looking down, judging you, while you are at the base thinking about how much further you still need to climb. Everytime something good happens they are saying things like “that’s not that impressive” and when something bad/undersirable happens they are saying “you see, I knew he wasn’t cut out for this”.</p> <p>This bottom of the mountain feeling can creep up at any point in the day, and it is always bound to cause some unneccesary anxiety, stress, and discomfort. The worst part is, it’s really difficult to admit how you are feeling because imposter syndrome is directly attached to how you think people perceive you.</p> <p>If you tell someone who has had relatively less “success” than you, it’s usually met with a “that’s not even bad” or “you are still in such a good spot don’t worry” - and thats completely fair. It’s like telling someone who ate fast food you didn’t like how your steak was cooked last night. Your bad day/time/experience could very well be someones good day/time/experience (definitely an exaggerated example).</p> <p>If you tell someone who has had more success than you, it’s usually met with a “I’ve been there before, its okay” or “just keep grinding you got it” - also completely fair. It’s a natural response to be sympathetic especially when you have been in someones shoes. However, to someone with imposter syndrome, its easy to forget that whoever is giving that advice got their success through some combination of hardwork, dedication, and luck. You assume that the more successful person got there due to sheer talent and that you just don’t have that talent.</p> <p>Its also important to note that the concessions from the people trying to help/understand are not bad - it’s just that it doesn’t help break the mindset of imposter syndrome. Being able to confide in someone regardless of how successful they are is a luxury and shouldn’t be taken for granted. My only caveat, from my own experience, is that it takes a lot of internal mental effort to begin viewing yourself/success in a different light. Make sure that you supplement confiding in others with your own self-reflection.</p> <p>I by no means have conquered my imposter syndrome, but over the years it has gotten significantly better by changing how I think about things.</p> <h2 id="how-did-we-get-here">How did we get here?</h2> <p>I read an unrelated quote recently that has really helped me internalize the uniqueness of the human experience.</p> <blockquote> <p>Your personal experiences with money make up maybe 0.00000001% of what’s happened in the world, but maybe 80% of how you think the world works. - Morgan Housel, Psychology of Money</p> </blockquote> <p>Take out the “with money” portion and focus on just the numbers for a second. By the nature of decision making, the probability you ended up where you are is incredibly small (way smaller than that 0.00000001% mark). That’s also the beauty about probability - no matter how improbable a sequence of events are, an outcome still occurs.</p> <p>To go off on a quick probability tangent to really drive home this point, imagine flipping a fair coin 1000 times. The most probable sequence (the expectation) would be 500 heads and 500 tails. The probability of seeing this sequence, despite it being the most probable, is only ~.02 or 2%. Raise the number of flips to 10000 and that number becomes .8% (the number keeps going down the more coins you flip).</p> <p>Assuming your life’s decisions were as easy as coin flips and you only made 10000 of them over your life time, the chances you end up in the most probable state is only .8%. Now consider the fact that you have made exponentially more than 10000 decisions, a majority of your decisions were not simple binary outcomes, decisions out of your control directly influenced your situation, and chances are you are not in the expected state of your decision making. The probability that you ended up where you are, whether it be by your own decisions or not, is so small we could not begin to comprehend the number.</p> <p>Side note and shameless plug: The primary area of ML that has to do with decision making, modelling human cognition/reward, and interacting with an environment is called reinforcement learning. It is a super interesting topic and one of my favorite things to learn about. I have a whole slew of blog posts about them, go check it out!</p> <p>All of this is to say: everyones situation is unique and extremely improbable. As much as we like to admire uber successful people and super talented individuals, their current position came from an equally improbable sequence of decisions. Same goes for people who have not been as fortunate. Comparing situations, no matter how similar they may appear to be, seems a little less intuitive. Letting 80% of how we view our surroundings (or whatever number you choose) be based on insanely small chances of being in our given situation is way too disproportionate.</p> <h2 id="am-i-actually-good-at-smash">Am I actually good at Smash?</h2> <p>The other day, my friends and I were having about Smash (Super Smash Bros Ultimate). I’d like to think I am decent at smash, but obviously not the best. It’s one of my favorite games and I have spent an embarrasing amount of hours playing over the years. I play with my one friend who is definitely better than me, and when we play 1v1 he beats me ~67% of the time. During this interaction, someone asked him if I was good at Smash and his answer was “yeah he is really <strong>**</strong>* good” to which I was like “really?”</p> <p>Even though I know my friend is humble and wouldnt openly call out how often he beats me, it was still a slight shock to hear that. Maybe he just values his skills so highly that a 33% win rate is enough in his eyes to be really good? I asked him about it and he plainly said “I just think you are good at the game.” This wasn’t a super satisfactory answer because my imposter brain immediately defaulted to “how can you think that?”, but it got me thinking about how hard it is to evaluate yourself.</p> <p>Depending on your values, there can be many different metics for how success is defined. These could be money, happiness, comfortability, etc., but the point is that there is no well defined metric to say “you are __ successful”. Going back to smash, I see how often I lose at smash and figure I am good but not great. For my friend, he takes how I play at face value and determines I’m really good. Because everyone has different values, it feels like perspective matters more than some universal or single metric.</p> <p>I started realizing that when I get feelings of imposter syndrome, I am anchoring my metric for success to a single aspect of another person. Something like “this person makes more money than me” or “this person has been on publications more than me”. All of these observations make me think that I am not good enough or should be doing more.</p> <p>Now, I think about how my success is tied to my own values, not single aspects. I know that despite my best efforts to learn more ML, publish more papers, work harder, etc. there is inevitably going to be people that have more success in those areas. That is okay! I am alligning my actions with my values and to me that matters more than comparison.</p> <h2 id="conclusion">Conclusion</h2> <p>I am not a psychologist or someone specialized in helping people conquer their imposter syndrome, I’m just someone who lives with it. If you resonate with the stories/sentiment, I just want you to know that you deserve the success you have, and I hope you have more! Some of those mindset changes really helped me and I hope it can help you too :grin:</p>]]></content><author><name></name></author><category term="non-technical"/><category term="experience"/><summary type="html"><![CDATA[How I navigate feeling like an imposter]]></summary></entry><entry><title type="html">How do pre-trained sentence embeddings work?</title><link href="https://esoba.github.io/blog/2024/how-do-pre-trained-embeddings-work/" rel="alternate" type="text/html" title="How do pre-trained sentence embeddings work?"/><published>2024-02-26T17:00:10+00:00</published><updated>2024-02-26T17:00:10+00:00</updated><id>https://esoba.github.io/blog/2024/how-do-pre-trained-embeddings-work</id><content type="html" xml:base="https://esoba.github.io/blog/2024/how-do-pre-trained-embeddings-work/"><![CDATA[<h2 id="intro">Intro</h2> <p>When I first got into NLP, my first couple projects involved building some basic classification, clustering, and RAG algorithms using <a href="https://www.sbert.net/">sentence embeddings</a> or <a href="https://platform.openai.com/docs/guides/embeddings">OpenAI embeddings</a>. I think this was a great start because it made me realize that <a href="">NLP isn’t much different than other ML areas</a>, but as I kept going down the NLP rabbit hole I realized I didn’t truly understand these embeddings.</p> <p>Looking back, I don’t blame myself too much for my lack of understanding because word embeddings are a lot more intuitive. Without diving into specifics (which can be found <a href="">here</a>), word embeddings are created by analyzing the relationships between words and surrounding context in a corpus. The output of a perfect word embedding model are vectors whose geometric similarity (like cosine similarity) are representative of their semantic similarity. Thus, we can expect words with similar meanings to be closer together in the vector space.</p> <p>Even though I understood how word embeddings worked, I struggled with sentences for a couple different reasons. These were some of my qualms:</p> <ul> <li>Can’t there be an infinite number of sentences constructed from words in a corpus?</li> <li>How can you embed paragraphs?</li> <li>Even if you used attention to update vector representations, how do you combine them?</li> </ul> <h2 id="pooling">Pooling</h2> <p>You may have heard of pooling before in other ML contexts, particularly in Computer Vision where pooling refers to downsampling convolutional kernel output (aka feature maps). You perform pooling for a couple different reasons, primarily to:</p> <ul> <li>Reduce dimensionality (makes computation faster)</li> <li>Promote translation invariance (makes network less sensitive to feature location)</li> <li>Abstract/Consolidate features (makes network focus on large patterns vs local details)</li> </ul> <p>In NLP pooling has a different meaning, but can be thought of in the same way as the CV case. Pooling is a method of converting a sequence of word embeddings into a singular embedding that represents primary features of the entire sequence (aka downsampling!). As opposed to Convolutional Neural Networks where pooling is applied after every convolution layer, NLP pooling is only applied at the very end of self-attention blocks. Sentence embeddings are just the output of the pooling operation on context-aware word embeddings (typically from the final attention layer/block).</p> <p>Like in CV, there are different pooling methods that can be employed to get a representative embedding that captures the meaning of a sentence. The most common method is CLS pooling. In this method, a special classification token (usually called <CLS> or <s>) is appended to the beginning of every sentence/phrase/sequence of tokens the model is trained on. This token goes through the attention mechanism and is trained like how all other tokens would be (with the exception that the CLS token is never masked). Thus, it also has a representation that can be updated with contextually aware information. The CLS token is like the innocent bystander of the attention process - it is updated with relevant information but not necessarily apart of the sentence itself. Because of this, the representation of the CLS token has been chosen as a way to pool all of the word embeddings from a sentence. The idea is that the CLS token contains the same sentence level representation without being biased by any local semantics. A little more on BERT, the go-to language model for explaining/performing the state of the art, can be found [here]().</s></CLS></p> <p>To go back to CV for a moment, this is a bit analogous to max pooling in the sense that there are other pooling options but research consistently uses one method. Some other methods for pooling in NLP include averaging all of the embeddings, taking the max value at every index across all embeddings, etc. I have not gone into full depth on if a particular pooling method is preferred for a particular usecase, but I suspect there wouldn’t be much of a difference (especially if the embedding is going to be fed into another neural network).</p> <p>NOTE: Because attention (at its most basic form) is a context-aware weighted average, CLS pooling can be thought of as a weighted average of token embeddings. What’s nice is that the computation/weights for that average fall out naturally by virtue of passing text through the model.</p> <p>NOTE: Not every language model has a CLS token with its pre-training objective, so if you care to know the behind the scenes on how the model you are using does its embedding I would suggest reading through HuggingFace/architecture diagrams.</p> <h2 id="answers-to-my-previous-qualms">Answers to my previous qualms</h2> <ol> <li>Can’t there be an infinite number of sentences constructed from words in a corpus?</li> </ol> <p>Technically yes if we define a sentence to be a length-unrestricted sequence of words (tokens). I can see the confusion where the thought process is “there are an infinite number of combinations so how can an infinite combination fit in one embedding space”. My counter to that is unlike word embeddings, they do not all sit in one space. There is no sentence embedding space like words, it is more so sentence embeddings are a function of embeddings that live in the word embedding space. That function is the pooling operation and it is dependent on the model you are getting embeddings from.</p> <ol> <li>How can you embed paragraphs?</li> </ol> <p>Same way you would a sentence (aka pooling). A period (what delimits sentences from each other) is just a token, there is nothing special about it from the perspective of a language model. Assuming the paragraph you want to embed is not larger than the context limit of the language model you are passing it to, the process would be the exact same for a singular sentence.</p> <ol> <li>Even if you used attention to update vector representations, how do you combine them?</li> </ol> <p>Depends on the pooling method you choose. The default method (most common) is just to take the embedding of the classification token (usually <cls> or <s>) from a sequence. The idea is that the classification token captures all of the relevant sequence level information and it's embedding is updated just like the rest of the word embeddings. Another technique is to average the embeddings of all the words in the sentence. There is no consensus on which method to choose, but for models trained on classification tasks its best to just use the cls representation.</s></cls></p>]]></content><author><name></name></author><category term="advanced"/><category term="NLP"/><category term="GenAI"/><summary type="html"><![CDATA[What does it mean to embed a sentence?]]></summary></entry><entry><title type="html">I don’t use SQL as much as I should</title><link href="https://esoba.github.io/blog/2024/i-dont-use-sql-but-i-should/" rel="alternate" type="text/html" title="I don’t use SQL as much as I should"/><published>2024-02-26T17:00:10+00:00</published><updated>2024-02-26T17:00:10+00:00</updated><id>https://esoba.github.io/blog/2024/i-dont-use-sql-but-i-should</id><content type="html" xml:base="https://esoba.github.io/blog/2024/i-dont-use-sql-but-i-should/"><![CDATA[<h2 id="intro">Intro</h2> <p>The title kind of speaks for itself, but I admittedly haven’t had many opportunities to work with SQL. Because I am in the ML field and also because my technical title for work is Data Scientist, it is assumed that I know SQL and use it on a day to day basis. The truth is, I am often looking at open source datasets or getting data handed to me (a bit privileged, I know). I say I know SQL, but really I only know the basics and don’t get to practice these skills very often. Luckily, I had some exposure to it in college and know Pandas pretty well so I can dig through some documentation and understand whats going on.</p> <p>The guide below is for people who are in my shoes - ML practicioners who should know SQL just by virtue of just being in the field. Almost every Data Science, Data Engineer, Machine Learning Engineer job posting I’ve seen has some sort of requirement on SQL, so even if you never use SQL it will be good to know.</p> <p>I got a lot of this material from random sources and YouTube videos (all will be linked below), but I think the best resource for my learning was <a href="https://selectstarsql.com/">select star sql</a>. I have no affiliation with the person who made this site, but going through his chapters was super intuitive and interactive. His book gives you the ability to run queries in real time (I’ll just be showing code snippets here) which really solidified my learning.</p> <p>Also because I need some examples to actually show SQL in action, I opted to test out the knowledge on a select number of hackerrank SQL questions. I am not a fan of companies using hackerrank to weed out applicants (which I talk about in a <a href="https://selectstarsql.com/">blog post</a>), but for the sake of building skills I think it is pretty useful (especially here). There will be some basic examples in the sections describing core concepts, and harder examples towards the end</p> <h2 id="different-types-of-sql">Different types of SQL</h2> <p>Whenver I see SQL in the wild, its always followed by some modifier: PostgreSQL, MySQL, SQLite (what I first learned in college), etc. I never really understood the difference, but its about time I (we) do.</p> <p>The way I have come to understand the differences is that SQL is a standardized language used to manage/manipulate relational databases. All of the other flavors (like the ones that I listed above) is a specific relational database management system (RDBMS) that uses SQL as its querying language. The different flavors can also provide additional features/functionalities specific to the database system.</p> <p>As one <a href="https://www.reddit.com/r/SQL/comments/e0ao31/newbie_what_the_difference_between_sql_and_mysql/">redditor</a> put it, SQL = the English language, MySQL (or any other flavor) = a novel that was written using the English language. In terms of the additional functionalities mentioned above, you can think of a huge chunk (lets call it 95%) of the base functionality being the same and 5% being flavor specific.</p> <p>For the guide, I am going to go through SQLite because it is the <a href="https://www.sqlite.org/mostdeployed.html">most widely deployed</a> and the one I am most familiar with. Assuming the information I just mentioned above is true, the knowledge below should be widely applicable to all flavors.</p> <h2 id="convention">Convention</h2> <p>If you see SQL in the wild, its typically written out in a pretty consistent (and human readable) format. The main conventions in SQL are the following:</p> <ul> <li>SQL specific keywords are capitalized, while database schema specific keywords (like table names) are lowercase. <ul> <li>Some flavors of SQL can be case sensitive, verify that the flavor doesn’t have a restriction on this</li> </ul> </li> <li>As long as you don’t smush two words together, SQL is not very sensitive to whitespace. Improve readability by putting each command on a new line</li> <li>Arithmetic stays in integer form unless .0 is added to the hardcoded numbers. A common trick is to multiply a number by 1.0 to make it decimal</li> <li>Strings are denoted by single quotes</li> <li>Semicolons at the end of queries separate individual queries from each other</li> </ul> <h2 id="basic-commands">Basic commands</h2> <p>SELECT:</p> <ul> <li>Indicates what you want to output</li> <li>The * wildcard means “all columns”</li> <li>You can make your own columns from some combination of other columns</li> <li>You can use the AS keyword to alias some column</li> </ul> <p>In pandas, this is analogous to df[‘column_name’] or just simple look up. If you wanted to combine different columns, you could index columns and do whatever operation you want. For example df[‘col1’] + df[‘col2’]. The aliasing comes from whatever you decide to name the resultant dataframe</p> <p>FROM:</p> <ul> <li>Indicates what table in the database should be used</li> <li>Technically don’t need the FROM block if nothing from the table is being used</li> </ul> <p>With pandas it is assumed that the dataframe you have is not connected to multiple potential dataframes so the FROM keyword does not have a good analog.</p> <p>WHERE:</p> <ul> <li>Indicates what filters should be applied before outputting</li> <li>Done with boolean indexing using operators like &gt;, = (not ==), etc.</li> <li>For strings, SQL supports a couple different, more powerful operators <ul> <li>Use the LIKE to do wildcard searches</li> <li>LIKE ‘%val’ means any combo of strings + val</li> <li>LIKE ‘_val’ means single character match + val</li> <li>See documenation for more like this</li> </ul> </li> <li>To determine is something is NULL, use the keyword IS (val IS null)</li> </ul> <p>This is equivalent to something like df[df[‘col1’] &lt; 5] except of course you can replace &lt; with any operator and 5 with any desired filter value. Like in pandas, it is up to you to make sure the operator + filter value you choose is in line with the datatype you are dealing with (for example if col1 was all string data, the expression above wouldnt make sense).</p> <p>NOTE: The mental model I like to keep for WHERE (or pandas boolean indexing) is for loops. I imagine every WHERE clause or df[df[‘col1’] &lt; 5] as being:</p> <ul> <li>for row in df (or table) if row meets clause output row else skip</li> </ul> <p><a href="https://www.hackerrank.com/challenges/japanese-cities-attributes/problem">EXAMPLE</a></p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="o">*</span> 
<span class="k">FROM</span> <span class="n">CITY</span> 
<span class="k">WHERE</span> <span class="n">countrycode</span> <span class="o">=</span> <span class="s1">'JPN'</span>
</code></pre></div></div> <p>ORDER BY:</p> <ul> <li>Indicates the ordering of output based on a colum</li> <li>You can order by multiple things by separating columns by commas</li> </ul> <p>In pandas, this is like calling the sort_values() method</p> <p>LIMIT:</p> <ul> <li>Indicates how many results should be output</li> </ul> <p>This is equivalent to making your query in pandas and adding [:limit number] or [limit number:]. Essentially indexing the final output.</p> <h2 id="aggregating">Aggregating</h2> <p>NOTE: The flavor of SQL you use may have different syntax than the aggregators here or additional aggregate functionality. Refer to the documentation for your specific flavor for details. For completion, below are some of the most common ones</p> <p>COUNT:</p> <ul> <li>Counts the number of non-null rows in a column</li> <li>Can use COUNT(*) to get the length of an entire table <ul> <li>Counts rows as long as any one of their columns is non-null</li> </ul> </li> </ul> <p>DISTINCT:</p> <ul> <li>Returns only unique parts of a column and no duplicates</li> </ul> <p>SUM/MIN/MAX/AVG:</p> <ul> <li>Does the aggregation specified</li> </ul> <p>LENGTH/LEN:</p> <ul> <li>Returns the number of characters in a string</li> </ul> <p><a href="https://www.hackerrank.com/challenges/weather-observation-station-18/">EXAMPLE</a></p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">ROUND</span><span class="p">(</span><span class="k">MAX</span><span class="p">(</span><span class="n">lat_n</span><span class="p">)</span> <span class="o">-</span> <span class="k">MIN</span><span class="p">(</span><span class="n">lat_n</span><span class="p">)</span> <span class="o">+</span> <span class="k">MAX</span><span class="p">(</span><span class="n">long_w</span><span class="p">)</span> <span class="o">-</span> <span class="k">MIN</span><span class="p">(</span><span class="n">long_w</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">FROM</span> <span class="n">STATION</span>
</code></pre></div></div> <p>NOTE: In most cases, if you try to output a column and an aggregation from SELECT you will get just one output. The column output will be some singular random output instead. This is because SQL doesn’t know how to output a singular value (which the aggregator returns) with multiple values from the specified column</p> <p>You can do aggregating either on the entire table, or a subset of the table (more info in the nested query section)</p> <h2 id="group-by">Group By</h2> <p>GROUP BY:</p> <ul> <li>Gathers identical column values to allow aggregators to be used on them</li> <li>Always comes after a WHERE block</li> </ul> <p>This is pretty much identical to pandas groupby.</p> <p>HAVING:</p> <ul> <li>Allows you to filter after output after performing group by logic</li> <li>Essentially the WHERE clause, except specifically for groupby outputs <ul> <li>WHERE happens before grouping and aggregation</li> </ul> </li> </ul> <p><a href="https://www.hackerrank.com/challenges/earnings-of-employees/problem">Example</a></p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">salary</span><span class="o">*</span><span class="n">months</span> <span class="k">as</span> <span class="n">tot_sal</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
<span class="k">FROM</span> <span class="n">employee</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">tot_sal</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">tot_sal</span> <span class="k">DESC</span>
<span class="k">LIMIT</span> <span class="mi">1</span>
</code></pre></div></div> <p>NOTES: GROUP BY breaks the NOTE above because if you SELECT a column and some aggregate when a groupby is introduced, the aggregate will be performed first allowing each column to have some number associated with it. This is of course assuming that you are consistent with what you are grouping by (For example, SELECT <strong>city</strong>, COUNT(city) FROM city_table GROUP BY <strong>city</strong>)</p> <p>If you group by multiple objects and there is nothing to display for a particular group, nothing will be returned. I think the best explanation for this can be found in the (decade age example)[https://selectstarsql.com/longtail.html]. Essentially if (group1, group2) has an aggregation that doesnt exist, it will not show up in the output.</p> <p>In pandas, you get some group by functionality out of the box. For example, the value_counts() method is really just a short hand for groupby(‘column’).size() (or .count() if you do not want to consider NaN values). I am not sure if you get the same OOB funcionality with SQL</p> <h2 id="nested-queriessubsets">Nested Queries/Subsets</h2> <p>CASE WHEN:</p> <ul> <li>A way to introduce if-else logic into SQL</li> <li>The syntax is CASE WHEN clause THEN result …(more statements here)… ElSE result END (END needed to conclude statement)</li> </ul> <p>You can use this to create subsets of the table in a specified way (you get to specify each case or if statement)</p> <p><a href="https://www.hackerrank.com/challenges/binary-search-tree-1/problem?isFullScreen=true">EXAMPLE</a>:</p> <p>Without CASE WHEN</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">N</span><span class="p">,</span> <span class="s1">'Leaf'</span>
<span class="k">FROM</span> <span class="n">BST</span>
<span class="k">WHERE</span> <span class="n">N</span> <span class="k">NOT</span> <span class="k">IN</span> <span class="p">(</span><span class="k">SELECT</span> <span class="k">DISTINCT</span> <span class="n">P</span> <span class="k">FROM</span> <span class="n">BST</span> <span class="k">WHERE</span> <span class="n">P</span> <span class="k">IS</span> <span class="k">NOT</span> <span class="k">NULL</span><span class="p">)</span>

<span class="k">UNION</span>

<span class="k">SELECT</span> <span class="n">N</span><span class="p">,</span> <span class="s1">'Inner'</span>
<span class="k">FROM</span> <span class="n">BST</span>
<span class="k">WHERE</span> <span class="n">N</span> <span class="k">IN</span> <span class="p">(</span><span class="k">SELECT</span> <span class="n">P</span> <span class="k">FROM</span> <span class="n">BST</span> <span class="k">WHERE</span> <span class="n">P</span> <span class="k">IS</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="k">AND</span> <span class="n">P</span> <span class="o">!=</span> <span class="p">(</span><span class="k">SELECT</span> <span class="n">N</span> <span class="k">FROM</span> <span class="n">BST</span> <span class="k">WHERE</span> <span class="n">P</span> <span class="k">is</span> <span class="k">null</span><span class="p">))</span>

<span class="k">UNION</span>

<span class="k">SELECT</span> <span class="n">N</span><span class="p">,</span> <span class="s1">'Root'</span>
<span class="k">FROM</span> <span class="n">BST</span>
<span class="k">WHERE</span> <span class="n">P</span> <span class="k">is</span> <span class="k">null</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">N</span><span class="p">;</span>
</code></pre></div></div> <p>WITH CASE WHEN</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">N</span><span class="p">,</span> 
<span class="k">CASE</span> 
    <span class="k">WHEN</span> <span class="n">P</span> <span class="k">IS</span> <span class="k">null</span> <span class="k">THEN</span> <span class="s1">'Root'</span>
    <span class="k">WHEN</span> <span class="n">N</span> <span class="k">NOT</span> <span class="k">IN</span> <span class="p">(</span><span class="k">SELECT</span> <span class="n">P</span> <span class="k">FROM</span> <span class="n">BST</span> <span class="k">WHERE</span> <span class="n">P</span> <span class="k">is</span> <span class="k">NOT</span> <span class="k">NULL</span><span class="p">)</span> <span class="k">THEN</span> <span class="s1">'Leaf'</span>
    <span class="k">ELSE</span> <span class="s1">'Inner'</span>
<span class="k">END</span>
    
<span class="k">FROM</span> <span class="n">BST</span> 
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">N</span><span class="p">;</span>
</code></pre></div></div> <p>Sometimes, you need to aggregate both within groups and across the entire dataset. Unfortunately, this cannot be done at the same time with a single query. To bypass this, you can create a query that generates a subset of the data, and wrap it in parenthesees to isolate it from the rest of the query.</p> <h2 id="joins">Joins</h2> <p>LEFT/RIGHT/OUTER/INNER JOIN:</p> <ul> <li>Joins two tables together based on the specified <a href="https://www.w3schools.com/sql/sql_join.asp">JOIN type</a></li> <li>By default, using the JOIN keyword will do an INNER JOIN</li> <li>You typically join on a key that is the same across different tables <ul> <li>FROM table1 JOIN table 2 on table1.uid = table2.uid</li> </ul> </li> <li>You can join a table with itself (Useful when you make subsets w/different columns)</li> <li>You can alias without using AS</li> <li>You can optionally join on another boolean operator (useful if you don’t care about duplicates)</li> </ul> <p><a href="https://www.hackerrank.com/challenges/asian-population/problem?isFullScreen=true">EXAMPLE</a>:</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="k">SUM</span><span class="p">(</span><span class="k">c</span><span class="p">.</span><span class="n">population</span><span class="p">)</span>
<span class="k">FROM</span> <span class="n">city</span> <span class="k">c</span>
<span class="k">JOIN</span> <span class="n">country</span> <span class="n">co</span> <span class="k">ON</span> <span class="k">c</span><span class="p">.</span><span class="n">countrycode</span> <span class="o">=</span> <span class="n">co</span><span class="p">.</span><span class="n">code</span>
<span class="k">WHERE</span> <span class="n">co</span><span class="p">.</span><span class="n">continent</span> <span class="o">=</span> <span class="s1">'Asia'</span>
</code></pre></div></div> <p>NOTE: As mentioned above, we aliased city and country without specifying AS</p> <h2 id="other-important-keywords">Other important keywords</h2> <p>RIGHT/LEFT:</p> <ul> <li>Extract characters from a string starting from either left or right - LEFT/RIGHT(“my_str”, # char to extract)</li> </ul> <p><a href="https://www.hackerrank.com/challenges/weather-observation-station-6/problem?isFullScreen=true">Example</a>:</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="k">DISTINCT</span> <span class="n">city</span>
<span class="k">FROM</span> <span class="n">station</span>
<span class="k">WHERE</span> <span class="k">LEFT</span><span class="p">(</span><span class="n">city</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">in</span> <span class="p">(</span><span class="s1">'a'</span><span class="p">,</span> <span class="s1">'e'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">,</span> <span class="s1">'o'</span><span class="p">,</span> <span class="s1">'u'</span><span class="p">)</span>
</code></pre></div></div> <p>SUBSTR:</p> <ul> <li>Extract a substring from a string - SUBSTR(“my_str”, start_position, # of extract characters)</li> <li>Can modify the above example to use substr instead of left</li> </ul> <p>CONCAT:</p> <ul> <li>Concatentate strings together - CONCAT (string1, string2, …)</li> </ul> <p><a href="https://www.hackerrank.com/challenges/the-pads/problem?isFullScreen=true">Example</a>:</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">CONCAT</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">'('</span><span class="p">,</span> <span class="k">LEFT</span><span class="p">(</span><span class="n">occupation</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">')'</span><span class="p">)</span> <span class="k">as</span> <span class="n">paren</span>
<span class="k">FROM</span> <span class="n">occupations</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">name</span><span class="p">;</span>

<span class="k">SELECT</span> <span class="n">CONCAT</span><span class="p">(</span><span class="s1">'There are a total of '</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">),</span> <span class="s1">' '</span><span class="p">,</span> <span class="k">LOWER</span><span class="p">(</span><span class="n">occupation</span><span class="p">),</span> <span class="s1">'s.'</span><span class="p">)</span>
<span class="k">FROM</span> <span class="n">occupations</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">occupation</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
</code></pre></div></div> <p>WITH:</p> <ul> <li>Define a subset of a table to reference later - WITH reference_name AS (Subtable query)</li> </ul> <p>UNION:</p> <ul> <li>Combine the result set of two or more SELECT statements</li> </ul> <p>BETWEEN:</p> <ul> <li>Operator that selects values within a given range - col BETWEEN val1 AND val2</li> </ul> <p><a href="https://www.hackerrank.com/challenges/the-report/problem?isFullScreen=true">EXAMPLE</a></p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">WITH</span> <span class="n">comb</span> <span class="k">AS</span> 
<span class="p">(</span><span class="k">SELECT</span> <span class="n">s</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="k">g</span><span class="p">.</span><span class="n">grade</span><span class="p">,</span> <span class="n">s</span><span class="p">.</span><span class="n">marks</span>
<span class="k">FROM</span> <span class="n">students</span> <span class="n">s</span>
<span class="k">LEFT</span> <span class="k">JOIN</span> <span class="n">grades</span> <span class="k">g</span> 
<span class="k">ON</span> <span class="n">s</span><span class="p">.</span><span class="n">marks</span> <span class="k">BETWEEN</span> <span class="k">g</span><span class="p">.</span><span class="n">min_mark</span> <span class="k">AND</span> <span class="k">g</span><span class="p">.</span><span class="n">max_mark</span><span class="p">)</span>

<span class="p">(</span><span class="k">SELECT</span> <span class="n">name</span><span class="p">,</span> <span class="n">grade</span><span class="p">,</span> <span class="n">marks</span>
<span class="k">FROM</span> <span class="n">comb</span>
<span class="k">WHERE</span> <span class="n">grade</span> <span class="o">&gt;=</span> <span class="mi">8</span><span class="p">)</span>

<span class="k">UNION</span> 

<span class="p">(</span><span class="k">SELECT</span> <span class="k">NULL</span> <span class="k">as</span> <span class="n">name</span><span class="p">,</span> <span class="n">grade</span><span class="p">,</span> <span class="n">marks</span>
<span class="k">FROM</span> <span class="n">comb</span>
<span class="k">WHERE</span> <span class="n">grade</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">)</span>

<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">grade</span> <span class="k">DESC</span><span class="p">,</span> <span class="n">name</span> <span class="k">ASC</span><span class="p">;</span>
</code></pre></div></div> <h2 id="more-examples">More Examples</h2> <p>Coming soon!</p> <h2 id="note-on-llms-for-sql">Note on LLMs for SQL</h2> <p>Because the base functionality of SQL is very close to written language, it is is the perfect candidate for a task that can be done with Large Language Models (LLMs). For the examples above in hackerrank, GPT-4 was able to reproduce most of the answers I wrote by just providing a description of the task and database schema. The ones that it did get wrong could easily be fixed by understanding all of the concepts above. There are also some LLMs out there that were fine-tuned</p> <h2 id="sources">Sources</h2> <p><a href="https://selectstarsql.com/">select star sql</a> <a href="https://www.youtube.com/watch?v=gm6tNK_iOHs&amp;list=PLR0triVyTrBWOLNu3ato7Y9hnGVyhTe1c">basic sql YT playlist</a> <a href="https://www.youtube.com/watch?v=zsjvFFKOm3c&amp;pp=ygUMc3FsIGZpcmVzaGlw">SQL in 100 seconds</a> <a href="https://www.youtube.com/watch?v=vpzO8QTrgbc&amp;pp=ygUOc3FsIGhhY2tlcnJhbms%3D">guy answers all hackerrank sql questions</a></p>]]></content><author><name></name></author><category term="basic"/><category term="cs-skills"/><summary type="html"><![CDATA[Intro SQL guide for people who should know SQL]]></summary></entry></feed>
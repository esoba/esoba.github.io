---
layout: post
title: LLMs in layman's term
date: 2024-02-26 17:00:10
description: Explaining LLMs with analogies
tags: NLP GenAI 
categories: non-technical
---
**Finalizing material, will be done soon!**

## Intro

Large Language Models (LLMs) are able to generate content by sampling from a learned probability distribution over possible next tokens. Even though modern LLMs have been trained on most of the internet and have billions of parameters, there is no real guarentee on an LLMs output. I like to equate this This has led to research in many areas such as hallucination detection, robustness, 

### Difference between uncertainty and confidence

### Why can't we use output probabilities to measure uncertainty?

When considering the fact that 

#### Entropy

### What evaluation metrics has research come up with? 

### Does asking an LLM to rate its own output work?

### 
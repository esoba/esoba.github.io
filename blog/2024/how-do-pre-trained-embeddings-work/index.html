<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How do pre-trained sentence embeddings work? | Elijah Soba </title> <meta name="author" content="Elijah Soba"> <meta name="description" content="What does it mean to embed a sentence?"> <meta name="keywords" content="elijah, elijah soba, esoba, esoba@umich.edu, elijah portfolio, machine learning blog, machine learning, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/dalle_logo1.png?c561aaa2e846c03dcbc2afc5f520dd26"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://esoba.github.io/blog/2024/how-do-pre-trained-embeddings-work/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Elijah</span> Soba </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about me </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/resources/">resources </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/Soba_Elijah_Resume.pdf">resume </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">How do pre-trained sentence embeddings work?</h1> <p class="post-meta"> February 26, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/tag/genai"> <i class="fa-solid fa-hashtag fa-sm"></i> GenAI</a>     ·   <a href="/blog/category/advanced"> <i class="fa-solid fa-tag fa-sm"></i> advanced</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="intro">Intro</h2> <p>When I first got into NLP, my first couple projects involved building some basic classification, clustering, and RAG algorithms using <a href="https://www.sbert.net/" rel="external nofollow noopener" target="_blank">sentence embeddings</a> or <a href="https://platform.openai.com/docs/guides/embeddings" rel="external nofollow noopener" target="_blank">OpenAI embeddings</a>. I think this was a great start because it made me realize that <a href="">NLP isn’t much different than other ML areas</a>, but as I kept going down the NLP rabbit hole I realized I didn’t truly understand these embeddings.</p> <p>Looking back, I don’t blame myself too much for my lack of understanding because word embeddings are a lot more intuitive. Without diving into specifics (which can be found <a href="">here</a>), word embeddings are created by analyzing the relationships between words and surrounding context in a corpus. The output of a perfect word embedding model are vectors whose geometric similarity (like cosine similarity) are representative of their semantic similarity. Thus, we can expect words with similar meanings to be closer together in the vector space.</p> <p>Even though I understood how word embeddings worked, I struggled with sentences for a couple different reasons. These were some of my qualms:</p> <ul> <li>Can’t there be an infinite number of sentences constructed from words in a corpus?</li> <li>How can you embed paragraphs?</li> <li>Even if you used attention to update vector representations, how do you combine them?</li> </ul> <h2 id="pooling">Pooling</h2> <p>You may have heard of pooling before in other ML contexts, particularly in Computer Vision where pooling refers to downsampling convolutional kernel output (aka feature maps). You perform pooling for a couple different reasons, primarily to:</p> <ul> <li>Reduce dimensionality (makes computation faster)</li> <li>Promote translation invariance (makes network less sensitive to feature location)</li> <li>Abstract/Consolidate features (makes network focus on large patterns vs local details)</li> </ul> <p>In NLP pooling has a different meaning, but can be thought of in the same way as the CV case. Pooling is a method of converting a sequence of word embeddings into a singular embedding that represents primary features of the entire sequence (aka downsampling!). As opposed to Convolutional Neural Networks where pooling is applied after every convolution layer, NLP pooling is only applied at the very end of self-attention blocks. Sentence embeddings are just the output of the pooling operation on context-aware word embeddings (typically from the final attention layer/block).</p> <p>Like in CV, there are different pooling methods that can be employed to get a representative embedding that captures the meaning of a sentence. The most common method is CLS pooling. In this method, a special classification token (usually called <cls> or <s>) is appended to the beginning of every sentence/phrase/sequence of tokens the model is trained on. This token goes through the attention mechanism and is trained like how all other tokens would be (with the exception that the CLS token is never masked). Thus, it also has a representation that can be updated with contextually aware information. The CLS token is like the innocent bystander of the attention process - it is updated with relevant information but not necessarily apart of the sentence itself. Because of this, the representation of the CLS token has been chosen as a way to pool all of the word embeddings from a sentence. The idea is that the CLS token contains the same sentence level representation without being biased by any local semantics. A little more on BERT, the go-to language model for explaining/performing the state of the art, can be found [here]().</s></cls></p> <p>To go back to CV for a moment, this is a bit analogous to max pooling in the sense that there are other pooling options but research consistently uses one method. Some other methods for pooling in NLP include averaging all of the embeddings, taking the max value at every index across all embeddings, etc. I have not gone into full depth on if a particular pooling method is preferred for a particular usecase, but I suspect there wouldn’t be much of a difference (especially if the embedding is going to be fed into another neural network).</p> <p>NOTE: Because attention (at its most basic form) is a context-aware weighted average, CLS pooling can be thought of as a weighted average of token embeddings. What’s nice is that the computation/weights for that average fall out naturally by virtue of passing text through the model.</p> <p>NOTE: Not every language model has a CLS token with its pre-training objective, so if you care to know the behind the scenes on how the model you are using does its embedding I would suggest reading through HuggingFace/architecture diagrams.</p> <h2 id="answers-to-my-previous-qualms">Answers to my previous qualms</h2> <ol> <li>Can’t there be an infinite number of sentences constructed from words in a corpus?</li> </ol> <p>Technically yes if we define a sentence to be a length-unrestricted sequence of words (tokens). I can see the confusion where the thought process is “there are an infinite number of combinations so how can an infinite combination fit in one embedding space”. My counter to that is unlike word embeddings, they do not all sit in one space. There is no sentence embedding space like words, it is more so sentence embeddings are a function of embeddings that live in the word embedding space. That function is the pooling operation and it is dependent on the model you are getting embeddings from.</p> <ol> <li>How can you embed paragraphs?</li> </ol> <p>Same way you would a sentence (aka pooling). A period (what delimits sentences from each other) is just a token, there is nothing special about it from the perspective of a language model. Assuming the paragraph you want to embed is not larger than the context limit of the language model you are passing it to, the process would be the exact same for a singular sentence.</p> <ol> <li>Even if you used attention to update vector representations, how do you combine them?</li> </ol> <p>Depends on the pooling method you choose. The default method (most common) is just to take the embedding of the classification token (usually <cls> or <s>) from a sequence. The idea is that the classification token captures all of the relevant sequence level information and it's embedding is updated just like the rest of the word embeddings. Another technique is to average the embeddings of all the words in the sentence. There is no consensus on which method to choose, but for models trained on classification tasks its best to just use the cls representation.</s></cls></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/custom-finetuning-example/">Custom fine-tuning example</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/i-dont-use-sql-but-i-should/">I don't use SQL as much as I should</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/a-note-on-imposter-syndrome/">A note on imposter syndrome</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/llms-in-layman-terms/">LLMs in layman's term</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/what-changes-are-made-to-transformers/">Common modifications to vanilla Transformers</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Elijah Soba. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: April 17, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Exploring LLM uncertainty | Elijah Soba </title> <meta name="author" content="Elijah Soba"> <meta name="description" content="How much can we trust the output of an LLM?"> <meta name="keywords" content="elijah, elijah soba, esoba, esoba@umich.edu, elijah portfolio, machine learning blog, machine learning, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/dalle_logo1.png?c561aaa2e846c03dcbc2afc5f520dd26"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://esoba.github.io/blog/2024/exploring-llm-uncertainty/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Elijah</span> Soba </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about me </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/resources/">resources </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/Soba_Elijah_Resume.pdf">resume </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Exploring LLM uncertainty</h1> <p class="post-meta"> February 26, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/tag/ai-trust-safety"> <i class="fa-solid fa-hashtag fa-sm"></i> AI-Trust/Safety</a>   <a href="/blog/tag/genai"> <i class="fa-solid fa-hashtag fa-sm"></i> GenAI</a>     ·   <a href="/blog/category/advanced"> <i class="fa-solid fa-tag fa-sm"></i> advanced</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="intro">Intro</h2> <p>Large Language Models (LLMs) are able to generate content by sampling from a learned probability distribution over possible next tokens. Even though modern LLMs have been trained on trillions of words (tokens) and have billions of parameters, there is no real guarentee on an LLMs output. To date, there is no way to fully measure how certain an LLM output is besides human verification. If we can understand what makes an LLM certain/uncertain in its output, it would have huge implications not only for LLM applications, but also for the publics trust in AI as a whole. Below, I’ll explore some concepts and why this is such a hard problem.</p> <h2 id="quick-note">Quick Note</h2> <p>There is a bunch of research into ML model uncertainty and I won’t be able to cover it all in this post. When doing research into this topic, I fell into countless rabit holes covering many areas related to probability theory and its role in Machine Learning. If you are like me and enjoy that deep dive into pure fundamentals, I think this <a href="https://www.gatsby.ucl.ac.uk/~balaji/balaji-uncertainty-talk-cifar-dlrl.pdf" rel="external nofollow noopener" target="_blank">slide deck</a> is a great place to start. I am also planning on doing a deep dive into Bayesian statistics and it’s role in ML so keep a look out for that!</p> <p>Also, a majority of this post will be related to classification uncertainty since LLMs are auto-regressive token <em>classifiers</em>.</p> <h2 id="what-exactly-is-uncertainty-in-ml">What exactly is uncertainty in ML?</h2> <blockquote> <p>All models are wrong, but models that know when they are wrong, are useful.</p> </blockquote> <p>Uncertainty measures how much we can trust the predictions from a model. There are two types of uncertainty in ML:</p> <ul> <li>epistemic: uncertainty from model (learned parameters)</li> <li>aleatoric: uncertainty from the data (stochasticity of observations)</li> </ul> <p>Epistemic is said to be reducible, meaning that in the presence of more data our model can do better at solving a given task. Assuming we had a model with infinite capacity (aka ability to learn any data pattern), our epistemic uncertainty would fall to 0 if we gave it infinite data. This is equivalent to saying if you overfit a model trained on every possible datapoint, you can always be certain your model will correctly classify that data. A model will have high epistemic uncertainty when looking at a datapoint that comes from a different distribution than the data it was trained on (and vice versa). This uncertainty can be measured by training multiple models on different subsets of data and measuring the variability of the predictions.</p> <p>add little blurb on how to measure this type of uncertainty</p> <p>For LLMs, epistemic uncertainty comes from not having seen enough content on a particular topic. For example, if I ask an LLM that has not been trained on arabic to translate a piece of text for me, it most likely will produce some bad output.</p> <p>Aleatoric is irreducible, meaning that even in the presence of infinite data there will still be uncertainty in our predictions. This is due to the inherent randomness of data. Typically, observations/data we get from our environment will be noisy and we will not be able to know how that noise is distributed ahead of time. Because we cannot account for these perturbations, we have to accept that our model may be subject to data that is out of distribution (OOD) from its training data.</p> <p>I think aleatoric is a little trickier to define for LLMs, but I like to think of it in terms of prompting. Prompts are our “input” to LLMs, so any vagueness or ambiguity will cause uncertainty to be high (even if we had a uber intelligent model). A question like “What am I wearing today?” would produce an answer with high uncertainty*. In general, random noise added to the prompt (ambiguity, typos, etc) can be considered alleatoric.</p> <blockquote> <p>*NOTE: To be honest I read this example in a paper, but don’t know if I agree with it 100%. Conceptually, it makes sense if we assume just pure text completion training, but most LLMs are alligned (via RLHF, DPO, ORPO, etc.) to return an answer along the lines of “I don’t know how to answer that” (which is technically correct). I think that gets into a deeper philosophical question about an LLMs capability to understanding their limitations.</p> </blockquote> <p>When people refer to uncertainty without any particular label (like epistemic or aleatoric), they are typically referring to total uncertainty:</p> <p>Total Uncertainty = Epistemic + Aleatoric</p> <p>Uncertainty estimates fall into 4 categories:</p> <ul> <li>Single deterministic methods: Measure uncertainty based single pass through model</li> <li>Ensemble methods: Measure uncertainty based on multiple models output</li> <li>Bayesian methods: Measure uncertainty based on stoachasticity of model</li> <li>Test time augmentation methods: Measure uncertainty by augmenting data</li> </ul> <p>Common evaluation metrics for uncertainty include:</p> <ul> <li>Calibration: Measures the agreement between predicted probabilities and observed frequencies <ul> <li>Of all the times a model predicted class X with probability P, what fraction did we observe class X?</li> </ul> </li> <li>Log liklihood: Measures how well the model fit the data it was trained on</li> <li>Entropy: Measures how random/surprising a models predictions are</li> </ul> <p>Research in this space concerns itself with estimating the different types of uncertainty, and coming up with a total uncertainty measure that is representative of the task a particular model is trying to solve.</p> <p>There are a couple things I think are important to note:</p> <p>There is a difference between model uncertainty and model performance. Model performance considers the label output of the model compared to some known labels and uncertainty considers whether we should be asking the model to provide labels in the first place. Its the difference between saying “this image is a dog” and “I am 90% sure this is a dog”</p> <p>The uncertainty you see from a models prediction are not always indicative of real life certainty. In fact, it has been shown models tend to be overconfident with their predictive probabilities. This is all to say that probabilities from a model alone are not enough to determine real world uncertainty. It is very possible for a model to be very confident in its prediction but in reality be completely wrong (especially when giving it OOD data).</p> <h2 id="generation-makes-things-a-little-tricky">Generation makes things a little tricky</h2> <p>I like to think of the generation task as a sequence of classifications or regressions (supervised tasks). For example, LLM text completions are sequences of auto-regressive token classifications, and image generations are sequences of pixel regressions. Generative ML makes uncertainty measures a little bit more difficult for a few different reasons:</p> <ul> <li>Uncertainty can pop up in many different places in the sequence</li> <li>OOD is much harder to define</li> <li>Aggregating uncertainty across a sequence</li> <li>Generative models have an extraordinary large number of model parameters</li> <li>etc.</li> </ul> <h2 id="recap-of-llm-problem-setup">Recap of LLM problem setup</h2> <p>LLMs are given an input prompt and generate an output sequence based on the instructions of the prompt. We consider the prompt a sequence of tokens $X = [x_1, x_2, … x_n]$, and the model returns another sequence of tokens $Y = [y1, y2, …y_m]$. The general decoding process is: $y_j = f([X, y_1, y_2, …])$. This means that the $j^{th}$ output token is a function of the prompt and the previously generated tokens. The function is itself the LLM that has learned to output a probability distribution over next possible tokens in a sequence.</p> <p>The function is parameterized by the parameters of the ML model (underlying transformer architecture) and parameters of the sampling. The parameters of the ML model are fixed based on the pre-training the LLM provider had done, while the sampling parameters can be dictated by the user. These parameters include:</p> <ul> <li>Temperature</li> <li>Top_p</li> <li>Top_k</li> <li>Max output tokens</li> <li>etc.</li> </ul> <p>What each of these parameters mean and how to effectively use those are included in another blog post.</p> <h3 id="how-do-we-classify-ood-data-for-llms">How do we classify OOD data for LLMs</h3> <p>OOD data for a classifier can be considered any input that does not belong to the predefined classes or deviates from the input distribution within those classes. For something like a dog vs cat classifier, OOD data could be a picture of a bird or a picture of a dog/cat its never seen before (for example, asking it to classify a chihuaha when it has only seen pictures of huskies).</p> <p>For LLMs, the “does not belong to predefined classes” definition doesn’t hold up as well* because the LLM has been trained to see all types of tokens. Put another way, the vocabulary is finite and it has seen every token in the vocabulary. Thus, OOD data in this case refers more to knowledge than individual tokens. Prompting with information the model has never seen before (or has been aligned on), is considered OOD.</p> <blockquote> <p>*Note: For the purposes of describing OOD data we’ll assume the LLM has been trained on the language it is being queried with. Asking an LLM that only knows English a question in Spanish would be considered OOD in the predefined classes sense, but we won’t consider that case because it is fairly rare.</p> </blockquote> <h2 id="why-cant-we-use-output-probabilities-to-measure-uncertainty">Why can’t we use output probabilities to measure uncertainty?</h2> <p>Because LLMs output token probabilities, it would seem reasonable that the probability of a sequence would be a good proxy for uncertainty. Or, at the very least, there would be some meaningful way to combine token probabilities to make a good uncertainty metric. Let’s start with the simplest case: a single token output. Consider the following example (and assume True/False are 1 token):</p> <p>Prompt: Given the following headline, determine if it is related to AI. Respond with only a true or false.</p> <p>Headline: Elon Musk sues OpenAI over mission statement</p> <p>Answer: True</p> <p>This case is similar to classification: we have an input that maps to a singular output. To measure total uncertainty, we can calculate the total uncertainty using token level entropy. I am planning a blog post on information theories role in ML, where I will dive in a little bit deeper to what this all means. Presented informally, the token level probability for this particular sequence would be:</p> <p>$H(t_{j}) = \sum \limits_{i=1}^m P(t_{j} = token_i)*\log P(t_{j} = token_i)$</p> <p>Where $t_{j}$ is the token we are predicting a position $j$, and $token_i$ is the $i^{th}$ token in an LLMs vocabulary. In slightly less technical terms, this is considered the average “surprise” of observing $t_j$ as the value that it is. The maximum entropy would be the case where every token is just as likely aka $P(t_{j} = token_i) = \frac{1}{m}$ because there would be no indication that any token is favored over another. We can use this fact to normalize token level entropy to get a representative value between 0-1. Considering it is normalized by the maxium entropy (uncertainty), 0 would represent a deterministic output (aka completely certain) and 1 would represent uniform output (aka completely uncertain).</p> <p>$H_{norm}(t_{j}) = \frac{H(t_{j})}{H_{max}}$</p> <p>For the particular case above, the normalized entropy* is <em>__</em>. We can see that the value is close to 0, meaning the model is fairly certain its prediction!</p> <p>Once we start to relax some of these assumptions, issues arise. First, let’s revise the prompt to let it know that we can accept the answers True, False, Yes, or No.</p> <p>Prompt: Given the following headline, determine if it is related to AI. Respond with a single label that is either true, false, yes, or no.</p> <p>Headline: Elon Musk sues OpenAI over mission statement</p> <p>Answer: True</p> <p>If we calculate the normalized token level entropy again, we get <em>__</em>. Even though we got the same answer from the LLM, it is more uncertain. This is because of semantics, and it is one of the main drivers for why uncertainty estimation is so difficult. Since True and Yes are both valid answers to the quesetion, the output token probability distribution is a lot more flat. We can see this below:</p> <p>Insert picture of probablities here</p> <p>This may seem like a contrived example, but imagine a scenario where instead of constraining the input to True, False, Yes, or No you asked something like “Give me a one word answer”. We would still see the same issue because many single words can answer the question. The point is that total uncertainty cannot be measured by entropy alone becuase there are many ground truth labels.</p> <p>Let’s relax the assumption again and not constrain the output to being within a pre-defined set.</p> <p>However, we cannot use that same confidence (probability) metric when we have an answer that is not constrained to a pre-defined set. Consider if the answer to the above question was instead something like:</p> <p>Prompt: Given the following headline, determine if it is related to AI.</p> <p>Headline: Elon Musk sues OpenAI over mission statement</p> <p>Answer: “Yes, this article is related to AI”</p> <p>Semantically, this is the same answer as the ones above. However, we now have multiple sources of uncertainty because there are multiple sequential predictions. One method we could do is find the token level entropies for each predicition and consolidate them to get some representative total uncertainty. For example:</p> <ul> <li>Average across all tokens to get an average token level uncertainty,</li> <li>Take the max value to say “we are only as certain as our most uncertain token”</li> <li>etc.</li> </ul> <p>Doing this introduces more complications because:</p> <ul> <li>Each aggregation would have tradeoffs</li> <li>Semantics are not well correlated with uncertainty</li> <li>Output sequence lengths are non-standard</li> <li>Only certain parts of a sequence are relevant for answering the prompt</li> </ul> <p>If we relax our problem completely to any arbitrary type of problem (not necessarily just classification) you can imagine OUR uncertainty in how to measure the models uncertainty gets bigger and bigger.</p> <h3 id="does-asking-an-llm-to-rate-its-own-output-work">Does asking an LLM to rate its own output work?</h3> <h3 id="what-evaluation-metrics-has-research-come-up-with">What evaluation metrics has research come up with?</h3> <p>###</p> <p>Sources https://arxiv.org/pdf/2307.10236.pdf https://arxiv.org/pdf/2307.10236.pdf https://www.watchful.io/blog/decoding-llm-uncertainties-for-better-predictability https://cookbook.openai.com/examples/using_logprobs https://arxiv.org/pdf/2307.15703.pdf https://imerit.net/blog/a-comprehensive-introduction-to-uncertainty-in-machine-learning-all-una/</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/i-dont-use-sql-but-i-should/">I don't use SQL as much as I should</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/llms-in-layman-terms/">LLMs in layman's term</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/how-do-pre-trained-embeddings-work/">How do pre-trained sentence embeddings work?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/a-note-on-imposter-syndrome/">A note on imposter syndrome</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Elijah Soba. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: April 23, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
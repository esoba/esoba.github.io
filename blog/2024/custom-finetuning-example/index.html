<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Custom fine-tuning example | Elijah Soba </title> <meta name="author" content="Elijah Soba"> <meta name="description" content="How can we fine-tune a model more explicitly?"> <meta name="keywords" content="elijah, elijah soba, esoba, esoba@umich.edu, elijah portfolio, machine learning blog, machine learning, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/dalle_logo1.png?c561aaa2e846c03dcbc2afc5f520dd26"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://esoba.github.io/blog/2024/custom-finetuning-example/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Elijah</span> Soba </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about me </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/resources/">resources </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/Soba_Elijah_Resume.pdf">resume </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Custom fine-tuning example</h1> <p class="post-meta"> February 26, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/tag/genai"> <i class="fa-solid fa-hashtag fa-sm"></i> GenAI</a>     ·   <a href="/blog/category/advanced"> <i class="fa-solid fa-tag fa-sm"></i> advanced</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="intro">Intro</h2> <p>When I first got into NLP, my first couple projects involved building some basic classification, clustering, and RAG algorithms using <a href="https://www.sbert.net/" rel="external nofollow noopener" target="_blank">sentence embeddings</a> or <a href="https://platform.openai.com/docs/guides/embeddings" rel="external nofollow noopener" target="_blank">OpenAI embeddings</a>. I think this was a great start because it made me realize that <a href="">NLP isn’t much different than other ML areas</a>, but as I kept going down the NLP rabbit hole I realized I didn’t truly understand these embeddings.</p> <p>Looking back, I don’t blame myself too much for my lack of understanding because word embeddings are a lot more intuitive. Without diving into specifics (which can be found <a href="">here</a>), word embeddings are created by analyzing the relationships between words and surrounding context in a corpus. The output of a perfect word embedding model are vectors whose geometric similarity (like cosine similarity) are representative of their semantic similarity. Thus, we can expect words with similar meanings to be closer together in the vector space.</p> <p>Even though I understood how word embeddings worked, I struggled with sentences for a couple different reasons. These were some of my qualms:</p> <ul> <li>Can’t there be an infinite number of sentences constructed from words in a corpus?</li> <li>How can you embed paragraphs?</li> <li>Even if you used attention to update vector representations, how do you combine them?</li> </ul> <h2 id="pooling">Pooling</h2> <p>You may have heard of pooling before in other ML contexts, particularly in Computer Vision where pooling refers to downsampling convolutional kernel output (aka feature maps). You perform pooling for a couple different reasons, primarily to:</p> <ul> <li>Reduce dimensionality (makes computation faster)</li> <li>Promote translation invariance (makes network less sensitive to feature location)</li> <li>Abstract/Consolidate features (makes network focus on large patterns vs local details)</li> </ul> <p>In NLP pooling has a different meaning, but can be thought of in the same way as the CV case. Pooling is a method of converting a sequence of word embeddings into a singular embedding that represents primary features of the entire sequence (aka downsampling!). As opposed to Convolutional Neural Networks where pooling is applied after every convolution layer, NLP pooling is only applied at the very end of self-attention blocks. Sentence embeddings are just the output of the pooling operation on context-aware word embeddings (typically from the final attention layer/block).</p> <h2 id="different-pooling-methods">Different pooling methods</h2> <p>Like in CV, there are different pooling methods that can be employed to get a representative embedding that captures the meaning of a sentence.</p> <h2 id="answers-to-my-previous-qualms">Answers to my previous qualms</h2> <ol> <li>Can’t there be an infinite number of sentences constructed from words in a corpus?</li> </ol> <p>Technically yes if we define a sentence to be a length-unrestricted sequence of words (tokens). I can see the confusion where the thought process is “there are an infinite number of combinations so how can an infinite combination fit in one embedding space”. My counter to that is unlike word embeddings, they do not all sit in one space. There is no sentence embedding space like words, it is more so sentence embeddings are a function of embeddings that live in the word embedding space. That function is the pooling operation and can be</p> <ol> <li>How can you embed paragraphs?</li> </ol> <p>Same way you would a sentence (aka pooling). A period (what delimits sentences from each other) is just a token, there is nothing special about it from the perspective of a language model. Assuming the paragraph you want to embed is not larger than the context limit of the language model you are passing it to, the process would be the exact same for a singular sentence.</p> <ol> <li>Even if you used attention to update vector representations, how do you combine them?</li> </ol> <p>Depends on the pooling method you choose. The default method (most common) is just to take the embedding of the classification token (usually <cls> or <s>) from a sequence. The idea is that the classification token captures all of the relevant sequence level information and it's embedding is updated just like the rest of the word embeddings. Another technique is to average the embeddings of all the words in the sentence. There is no consensus on which method to choose, but for models trained on classification tasks its best to just use the cls representation</s></cls></p> <p>My original thought process</p> <h3 id="uncertainty-vs-confidence-vs-calibration">Uncertainty vs Confidence vs Calibration</h3> <h3 id="difference-between-uncertainty-and-hallucination-detection">Difference between uncertainty and hallucination detection</h3> <h3 id="why-cant-we-use-output-probabilities-to-measure-uncertainty">Why can’t we use output probabilities to measure uncertainty?</h3> <p>Because LLMs output token probabilities, it would seem reasonable that the probability of a sequence would be a good proxy for uncertainty. Or, at the very least, there would be some meaningful way to combine token probabilities to make a good uncertainty metric. With a singular token output, this could work. Consider the following example:</p> <blockquote> <p>Enter diagram for LLM output here</p> </blockquote> <p>In this simple example, we ask the LLM a yes or no question. We can look at the probability that the output token is “yes” and say</p> <h4 id="token-probability-entropy">Token Probability Entropy</h4> <p>E</p> <h3 id="what-evaluation-metrics-has-research-come-up-with">What evaluation metrics has research come up with?</h3> <h3 id="does-asking-an-llm-to-rate-its-own-output-work">Does asking an LLM to rate its own output work?</h3> <p>###</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/how-do-pre-trained-embeddings-work/">How do pre-trained sentence embeddings work?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/i-dont-use-sql-but-i-should/">I don't use SQL as much as I should</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/encoder-vs-decoder-transformers/">Difference between Encoder and Decoder Transformers</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/what-changes-are-made-to-transformers/">Common modifications to vanilla Transformers</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/llms-in-layman-terms/">LLMs in layman's term</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Elijah Soba. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: April 09, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>